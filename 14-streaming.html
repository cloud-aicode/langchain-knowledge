<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æµå¼å¤„ç† - LangChain 1.0 çŸ¥è¯†ç‚¹</title>
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body class="chapter-streaming">
    <button class="theme-toggle">ğŸŒ“</button>
    <button class="mobile-menu-btn" onclick="toggleSidebar()">â˜°</button>

    <div class="app-container">
        <nav class="sidebar">
            <div class="sidebar-header">
                <h1>LangChain 1.0</h1>
                <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 5px;">çŸ¥è¯†ç‚¹</p>
            </div>
            <div class="sidebar-nav"></div>
        </nav>

        <main class="main-content">
            <div class="content">
                <h1>æµå¼å¤„ç†</h1>
                <p class="subtitle">å®æ—¶è¾“å‡º LLM å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒ</p>

                <h2>ä¸ºä»€ä¹ˆéœ€è¦æµå¼è¾“å‡ºï¼Ÿ</h2>
                <p>ä¼ ç»Ÿ LLM è°ƒç”¨éœ€è¦ç­‰å¾…å®Œæ•´å“åº”ç”Ÿæˆï¼Œè¿™å¯èƒ½éœ€è¦å‡ ç§’åˆ°å‡ åç§’ã€‚æµå¼è¾“å‡ºè®©å†…å®¹é€å­—ç¬¦ï¼ˆæˆ–é€ tokenï¼‰å®æ—¶æ˜¾ç¤ºï¼š</p>
                <ul>
                    <li><strong>æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ</strong>ï¼šç”¨æˆ·ç«‹å³çœ‹åˆ°è¾“å‡ºï¼Œæ„Ÿè§‰å“åº”æ›´å¿«</li>
                    <li><strong>æ›´ä½çš„å»¶è¿Ÿæ„Ÿ</strong>ï¼šé¦–å­—å»¶è¿Ÿï¼ˆTTFFï¼‰æ¯”å®Œæ•´å“åº”æ—¶é—´çŸ­å¾—å¤š</li>
                    <li><strong>å®æ—¶äº¤äº’</strong>ï¼šå¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´</li>
                    <li><strong>é•¿è¾“å‡ºå‹å¥½</strong>ï¼šç”Ÿæˆé•¿æ–‡æœ¬æ—¶ç”¨æˆ·ä¸ä¼šä¸€ç›´ç­‰å¾…</li>
                </ul>

                <div class="tip" data-label="æç¤º">
                    æµå¼è¾“å‡ºå°±åƒçœ‹ç›´æ’­è§†é¢‘â€”â€”å†…å®¹ä¸€è¾¹ç”Ÿæˆä¸€è¾¹æ’­æ”¾ï¼Œè€Œä¸æ˜¯ç­‰æ•´ä¸ªè§†é¢‘ä¸‹è½½å®Œæˆåå†è§‚çœ‹ã€‚
                </div>

                <h2>æµå¼å¤„ç†æ¶æ„</h2>

                <pre class="mermaid">graph LR
    A[ç”¨æˆ·è¾“å…¥] --> B[LLM å¼€å§‹ç”Ÿæˆ]
    B --> C[Token 1]
    C --> D[Token 2]
    D --> E[Token 3]
    E --> F[...æ›´å¤š Token]
    F --> G[æœ€ç»ˆ Token]

    C --> H[å®æ—¶æ˜¾ç¤º]
    D --> H
    E --> H
    F --> H
    G --> H

    style H fill:#c8e6c9</code></pre>

                <h2>åŸºç¡€æµå¼è¾“å‡º</h2>

                <h3>åŒæ­¥æµå¼è¾“å‡º</h3>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI

# åˆ›å»ºæ”¯æŒæµå¼çš„ LLM
llm = ChatOpenAI(model="gpt-4o")

# æ–¹æ³•1ï¼šä½¿ç”¨ stream() æ–¹æ³•
print("æµå¼è¾“å‡º: ")
for chunk in llm.stream("è®²ä¸€ä¸ªå…³äº Python çš„ç®€çŸ­æ•…äº‹"):
    # æ¯ä¸ª chunk æ˜¯ä¸€ä¸ª AIMessageChunk
    print(chunk.content, end="", flush=True)

print("\n\nå®Œæˆ!")

# æ–¹æ³•2ï¼šä½¿ç”¨ invoke + streaming config
from langchain_core.callbacks import StdOutCallbackHandler

# è¿™ä¼šæ‰“å°æ¯ä¸ª token
response = llm.invoke(
    "è®²ä¸€ä¸ªå…³äº Python çš„ç®€çŸ­æ•…äº‹",
    config={"callbacks": [StdOutCallbackHandler()]}
)</code></pre>
                </div>

                <h3>å¼‚æ­¥æµå¼è¾“å‡º</h3>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">import asyncio
from langchain_openai import AsyncChatOpenAI

async def stream_example():
    # åˆ›å»ºå¼‚æ­¥ LLM
    llm = AsyncChatOpenAI(model="gpt-4o")

    print("å¼‚æ­¥æµå¼è¾“å‡º:")
    async for chunk in llm.astream("ç”¨ä¸‰å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½"):
        print(chunk.content, end="", flush=True)

    print("\n\nå®Œæˆ!")

# è¿è¡Œ
asyncio.run(stream_example())</code></pre>
                </div>

                <h2>åœ¨é“¾ä¸­ä½¿ç”¨æµå¼è¾“å‡º</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºé“¾
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç®€çŸ­æ•…äº‹")
llm = ChatOpenAI(model="gpt-4o")
chain = prompt | llm

# æµå¼æ‰§è¡Œé“¾
print("é“¾å¼æµå¼è¾“å‡º:\n")
for chunk in chain.stream({"topic": "é‡å­è®¡ç®—"}):
    # chunk æ˜¯ AIMessageChunk
    print(chunk.content, end="", flush=True)

print("\nå®Œæˆ!")

# ä¹Ÿå¯ä»¥ä½¿ç”¨ astream (å¼‚æ­¥)
async def stream_chain():
    async for chunk in chain.astream({"topic": "åŒºå—é“¾"}):
        print(chunk.content, end="", flush=True)

asyncio.run(stream_chain())</code></pre>
                </div>

                <h2>æµå¼è¾“å‡ºçš„æ•°æ®ç»“æ„</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# æŸ¥çœ‹æµå¼è¾“å‡ºçš„ chunk ç»“æ„
for chunk in llm.stream("ä½ å¥½"):
    print(f"å†…å®¹: {repr(chunk.content)}")
    print(f"ç±»å‹: {type(chunk)}")
    print(f"æ˜¯å¦æœ‰å“åº”å…ƒæ•°æ®: {chunk.response_metadata}")
    print(f"ID: {chunk.id}")
    print("---")

# è¾“å‡ºç¤ºä¾‹:
# å†…å®¹: 'ä½ '
# ç±»å‹: <class 'langchain_core.messages.ai.AIMessageChunk'>
# å“åº”å…ƒæ•°æ®: {...}
# ID: 'chatcmpl-...'
# ---
# å†…å®¹: 'å¥½'
# ---</code></pre>
                </div>

                <h2>å¤„ç†æµå¼è¾“å‡ºçš„ Token ç´¯ç§¯</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# æ–¹å¼1ï¼šç®€å•ç´¯ç§¯
full_response = ""
for chunk in llm.stream("ç”¨ä¸‰å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ "):
    full_response += chunk.content
    print(chunk.content, end="", flush=True)

print(f"\n\nå®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")

# æ–¹å¼2ï¼šä½¿ç”¨ç´¯åŠ å™¨ï¼ˆæ›´å®‰å…¨ï¼‰
from langchain_core.messages import AIMessageChunk

accumulated_message = None
for chunk in llm.stream("ç”¨ä¸‰å¥è¯ä»‹ç»æ·±åº¦å­¦ä¹ "):
    if accumulated_message is None:
        accumulated_message = chunk
    else:
        accumulated_message = accumulated_message + chunk

print(f"\nå®Œæ•´å†…å®¹:\n{accumulated_message.content}")

# æ–¹å¼3ï¼šè·å–å®Œæ•´å“åº”åç»§ç»­å¤„ç†
stream = llm.stream("åˆ—å‡º Python çš„5ä¸ªç‰¹æ€§")

# æ”¶é›†æ‰€æœ‰ chunks
chunks = []
for chunk in stream:
    chunks.append(chunk)
    print(chunk.content, end="", flush=True)

# åˆå¹¶è·å–å®Œæ•´æ¶ˆæ¯
full_message = chunks[0]
for chunk in chunks[1:]:
    full_message = full_message + chunk

print(f"\n\nToken ä½¿ç”¨: {full_message.response_metadata.get('token_usage', {})}")</code></pre>
                </div>

                <h2>æµå¼è¾“å‡ºä¸çŠ¶æ€ç®¡ç†</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from typing import AsyncIterator

class StreamingChat:
    """æ”¯æŒæµå¼è¾“å‡ºçš„èŠå¤©ç±»"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o")
        self.history = []

    async def astream_chat(self, message: str) -> AsyncIterator[str]:
        """å¼‚æ­¥æµå¼èŠå¤©"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.history.append({"role": "user", "content": message})

        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        context = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in self.history
        ])

        # æµå¼ç”Ÿæˆå“åº”
        full_response = ""
        async for chunk in self.llm.astream(context):
            content = chunk.content
            full_response += content
            yield content

        # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
        self.history.append({"role": "assistant", "content": full_response})

    def get_history(self):
        """è·å–å¯¹è¯å†å²"""
        return self.history

# ä½¿ç”¨ç¤ºä¾‹
async def chat_example():
    chat = StreamingChat()

    print("åŠ©æ‰‹: ", end="")
    async for token in chat.astream_chat("ä½ å¥½ï¼"):
        print(token, end="", flush=True)
    print()

    print("åŠ©æ‰‹: ", end="")
    async for token in chat.astream_chat("ä½ åˆšæ‰è¯´äº†ä»€ä¹ˆï¼Ÿ"):
        print(token, end="", flush=True)
    print()

    # æŸ¥çœ‹å†å²
    import json
    print("\nå¯¹è¯å†å²:")
    print(json.dumps(chat.get_history(), ensure_ascii=False, indent=2))

asyncio.run(chat_example())</code></pre>
                </div>

                <h2>æµå¼è¾“å‡ºæ—¶åºå›¾</h2>

                <pre class="mermaid">sequenceDiagram
    participant Client
    participant LangChain
    participant LLM

    Client->>LangChain: stream("...")
    LangChain->>LLM: å‘èµ·è¯·æ±‚

    loop æ¯ä¸ª Token
        LLM-->>LangChain: Token chunk
        LangChain-->>Client: yield chunk
        Client->>Client: æ›´æ–° UI
    end

    LLM-->>LangChain: å®Œæˆ
    LangChain-->>Client: æµç»“æŸ

    Note over Client: æ•´ä¸ªè¿‡ç¨‹ä¸­ç”¨æˆ·<br/>çœ‹åˆ°å†…å®¹é€å­—å‡ºç°</code></pre>

                <h2>ä¸ FastAPI é›†æˆ</h2>
                <p>åœ¨ Web åº”ç”¨ä¸­ä½¿ç”¨æµå¼è¾“å‡ºï¼š</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
import asyncio

app = FastAPI()
llm = ChatOpenAI(model="gpt-4o")

async def generate_stream(prompt: str):
    """ç”Ÿæˆæµå¼å“åº”"""
    async for chunk in llm.astream(prompt):
        # å‘é€æ¯ä¸ª chunk
        yield chunk.content

@app.get("/chat")
async def chat_endpoint(message: str):
    """æµå¼èŠå¤©ç«¯ç‚¹"""
    return StreamingResponse(
        generate_stream(message),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )

# ä½¿ç”¨ SSE (Server-Sent Events) æ ¼å¼
async def generate_sse(prompt: str):
    """ç”Ÿæˆ SSE æ ¼å¼çš„æµ"""
    async for chunk in llm.astream(prompt):
        # SSE æ ¼å¼: data: <content>\n\n
        yield f"data: {chunk.content}\n\n"

    # å‘é€ç»“æŸæ ‡è®°
    yield "data: [DONE]\n\n"

@app.get("/chat/sse")
async def chat_sse_endpoint(message: str):
    """SSE æµå¼èŠå¤©ç«¯ç‚¹"""
    return StreamingResponse(
        generate_sse(message),
        media_type="text/event-stream"
    )</code></pre>
                </div>

                <h2>å‰ç«¯ SSE æ¥æ”¶ç¤ºä¾‹</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">javascript</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-javascript">// ä½¿ç”¨ JavaScript æ¥æ”¶ SSE æµ
async function streamChat(message) {
    const response = await fetch(`/chat/sse?message=${encodeURIComponent(message)}`);
    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    const outputElement = document.getElementById('output');
    outputElement.textContent = '';

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = line.slice(6);
                if (data === '[DONE]') {
                    return;
                }
                outputElement.textContent += data;
                // è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
                outputElement.scrollTop = outputElement.scrollHeight;
            }
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
streamChat('è®²ä¸€ä¸ªå…³äº Python çš„æ•…äº‹');</code></pre>
                </div>

                <h2>æµå¼è¾“å‡ºçš„å¸¸è§æ¨¡å¼</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o")

# æ¨¡å¼1ï¼šå¸¦å‰ç¼€çš„æµå¼è¾“å‡º
async def stream_with_prefix():
    prefix = "åŠ©æ‰‹: "
    print(prefix, end="", flush=True)

    async for chunk in llm.astream("ä½ å¥½"):
        print(chunk.content, end="", flush=True)

    print()  # æ¢è¡Œ

# æ¨¡å¼2ï¼šæµå¼è¾“å‡ºåˆ°æ–‡ä»¶
async def stream_to_file():
    with open("output.txt", "w", encoding="utf-8") as f:
        async for chunk in llm.astream("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"):
            # åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            f.write(chunk.content)
            print(chunk.content, end="", flush=True)

# æ¨¡å¼3ï¼šåˆ†æ‰¹æµå¼è¾“å‡ºï¼ˆæ¯ N ä¸ª token åˆ·æ–°ä¸€æ¬¡ï¼‰
async def stream_in_batches(batch_size=5):
    batch = []
    async for chunk in llm.astream("è¯¦ç»†ä»‹ç» LangChain"):
        batch.append(chunk.content)
        if len(batch) >= batch_size:
            print("".join(batch), end="", flush=True)
            batch = []

    # æ‰“å°å‰©ä½™å†…å®¹
    if batch:
        print("".join(batch), end="", flush=True)

# æ¨¡å¼4ï¼šå¸¦è¶…æ—¶çš„æµå¼è¾“å‡º
import asyncio

async def stream_with_timeout(timeout=30):
    try:
        async for chunk in asyncio.as_completed(
            [llm.astream("è®²ä¸€ä¸ªé•¿æ•…äº‹")],
            timeout=timeout
        ):
            async for c in chunk:
                print(c.content, end="", flush=True)
    except asyncio.TimeoutError:
        print("\n[è¶…æ—¶ï¼Œåœæ­¢ç”Ÿæˆ]")</code></pre>
                </div>

                <h2>æœ€ä½³å®è·µ</h2>

                <div class="practice" data-label="å®è·µå»ºè®®">
                    <h3>1. æ€»æ˜¯åˆ·æ–°è¾“å‡ºç¼“å†²åŒº</h3>
                    <ul>
                        <li>ä½¿ç”¨ <code>flush=True</code> ç¡®ä¿ç«‹å³æ˜¾ç¤º</li>
                        <li>åœ¨ Web åº”ç”¨ä¸­ç¦ç”¨æœåŠ¡å™¨ç¼“å†²</li>
                        <li>é…ç½®æ­£ç¡®çš„ HTTP å¤´</li>
                    </ul>

                    <h3>2. å¤„ç†æµä¸­æ–­</h3>
                    <ul>
                        <li>ç½‘ç»œä¸­æ–­æ—¶ä¼˜é›…å¤„ç†</li>
                        <li>ä¿å­˜å·²ç”Ÿæˆçš„å†…å®¹</li>
                        <li>æä¾›é‡è¯•æœºåˆ¶</li>
                    </ul>

                    <h3>3. å‰ç«¯ä¼˜åŒ–</h3>
                    <ul>
                        <li>ä½¿ç”¨é€‚å½“çš„ç¼“å†²ç­–ç•¥</li>
                        <li>å®ç°æ‰“å­—æœºæ•ˆæœ</li>
                        <li>æ”¯æŒåœæ­¢ç”Ÿæˆ</li>
                    </ul>

                    <h3>4. æ€§èƒ½è€ƒè™‘</h3>
                    <ul>
                        <li>å¼‚æ­¥å¤„ç†é¿å…é˜»å¡</li>
                        <li>åˆç†è®¾ç½®æ‰¹å¤„ç†å¤§å°</li>
                        <li>ç›‘æ§å†…å­˜ä½¿ç”¨</li>
                    </ul>

                    <h3>5. é”™è¯¯å¤„ç†</h3>
                    <ul>
                        <li>æ•è·æµå¼è¿‡ç¨‹ä¸­çš„å¼‚å¸¸</li>
                        <li>æä¾›æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯</li>
                        <li>å®ç°è‡ªåŠ¨é‡è¿</li>
                    </ul>
                </div>

                <div class="exercise-section">
                    <h3>âœï¸ ç»ƒä¹ é¢˜</h3>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            1. å“ªä¸ªæ–¹æ³•ç”¨äºåŒæ­¥æµå¼è¾“å‡ºï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. astream()</div>
                            <div class="option correct-option">B. stream()</div>
                            <div class="option">C. invoke()</div>
                            <div class="option">D. batch()</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            2. æµå¼è¾“å‡ºçš„æ¯ä¸ª chunk æ˜¯ä»€ä¹ˆç±»å‹ï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. str</div>
                            <div class="option correct-option">B. AIMessageChunk</div>
                            <div class="option">C. dict</div>
                            <div class="option">D. list</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            3. SSE æ˜¯ä»€ä¹ˆçš„ç¼©å†™ï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. Simple Service Endpoint</div>
                            <div class="option">B. Synchronous Server Events</div>
                            <div class="option correct-option">C. Server-Sent Events</div>
                            <div class="option">D. Single Stream Engine</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">ä»£ç å¡«ç©º</span>
                            4. è¡¥å…¨ä»£ç ï¼šå®ç°å¼‚æ­¥æµå¼è¾“å‡º
                        </div>
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-language">python</span>
                                <button class="copy-btn">å¤åˆ¶</button>
                            </div>
                            <pre><code class="language-python">async def stream_response():
    llm = AsyncChatOpenAI(model="gpt-4o")
    async for chunk in llm.____("ä½ å¥½"):
        print(chunk.____, end="", flush=True)</code></pre>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">ç¼–ç¨‹é¢˜</span>
                            5. åˆ›å»ºä¸€ä¸ªæµå¼èŠå¤©ç±»ï¼Œæ”¯æŒå¤šè½®å¯¹è¯å¹¶ä¿å­˜å†å²è®°å½•
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">åœºæ™¯é¢˜</span>
                            6. ä½ æ­£åœ¨æ„å»ºä¸€ä¸ªå®æ—¶çš„ AI å†™ä½œåŠ©æ‰‹ï¼Œç”¨æˆ·è¾“å…¥æç¤ºè¯åï¼Œç³»ç»Ÿéœ€è¦æµå¼ç”Ÿæˆæ–‡ç« å†…å®¹ã€‚è¿˜éœ€è¦æ”¯æŒç”¨æˆ·éšæ—¶åœæ­¢ç”Ÿæˆã€‚ä½ ä¼šå¦‚ä½•è®¾è®¡è¿™ä¸ªç³»ç»Ÿï¼Ÿ
                        </div>
                    </div>
                </div>

                <div class="chapter-nav">
                    <a href="13-callbacks.html">â† ä¸Šä¸€ç« ï¼šå›è°ƒç³»ç»Ÿ</a>
                    <a href="15-tracing.html" class="next">ä¸‹ä¸€ç« ï¼šé“¾è·¯è¿½è¸ª â†’</a>
                </div>
            </div>
        </main>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script src="assets/js/main.js"></script>
</body>
</html>