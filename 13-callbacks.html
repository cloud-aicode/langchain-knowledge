<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å›è°ƒç³»ç»Ÿ - LangChain 1.0 çŸ¥è¯†ç‚¹</title>
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body class="chapter-callbacks">
    <button class="theme-toggle">ğŸŒ“</button>
    <button class="mobile-menu-btn" onclick="toggleSidebar()">â˜°</button>

    <div class="app-container">
        <nav class="sidebar">
            <div class="sidebar-header">
                <h1>LangChain 1.0</h1>
                <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 5px;">çŸ¥è¯†ç‚¹</p>
            </div>
            <div class="sidebar-nav"></div>
        </nav>

        <main class="main-content">
            <div class="content">
                <h1>å›è°ƒç³»ç»Ÿ</h1>
                <p class="subtitle">ç›‘å¬å’Œå“åº” LangChain è¿è¡Œæ—¶äº‹ä»¶</p>

                <h2>ä»€ä¹ˆæ˜¯å›è°ƒç³»ç»Ÿï¼Ÿ</h2>
                <p>å›è°ƒç³»ç»Ÿè®©ä½ èƒ½å¤Ÿåœ¨ LangChain ç»„ä»¶è¿è¡Œçš„ä¸åŒé˜¶æ®µæ‹¦æˆªå’Œå“åº”äº‹ä»¶ã€‚è¿™å°±åƒåœ¨ç¨‹åºè¿è¡Œæ—¶è®¾ç½®"ç›‘å¬å™¨"ï¼Œå½“ç‰¹å®šäº‹ä»¶å‘ç”Ÿæ—¶æ‰§è¡Œè‡ªå®šä¹‰é€»è¾‘ï¼š</p>
                <ul>
                    <li><strong>æ—¥å¿—è®°å½•</strong>ï¼šè®°å½•æ‰€æœ‰çš„è¾“å…¥è¾“å‡º</li>
                    <li><strong>æ€§èƒ½ç›‘æ§</strong>ï¼šè·Ÿè¸ª token ä½¿ç”¨å’Œæ‰§è¡Œæ—¶é—´</li>
                    <li><strong>è°ƒè¯•</strong>ï¼šæŸ¥çœ‹æ¯ä¸€æ­¥çš„æ‰§è¡Œç»†èŠ‚</li>
                    <li><strong>è‡ªå®šä¹‰å¤„ç†</strong>ï¼šåœ¨ç‰¹å®šäº‹ä»¶è§¦å‘æ—¶æ‰§è¡Œæ“ä½œ</li>
                </ul>

                <div class="tip" data-label="æç¤º">
                    å›è°ƒç³»ç»Ÿç±»ä¼¼äº Node.js çš„ä¸­é—´ä»¶æˆ– Express çš„æ‹¦æˆªå™¨â€”â€”å®ƒè®©ä½ åœ¨æ•°æ®æµåŠ¨çš„ä¸åŒèŠ‚ç‚¹"æ’å…¥"è‡ªå·±çš„ä»£ç ã€‚
                </div>

                <h2>å›è°ƒäº‹ä»¶ç”Ÿå‘½å‘¨æœŸ</h2>

                <pre class="mermaid">sequenceDiagram
    participant User
    participant Chain
    participant Callback
    participant LLM

    User->>Chain: invoke()
    Chain->>Callback: on_chain_start()
    Chain->>LLM: è°ƒç”¨æ¨¡å‹
    Callback->>Callback: on_llm_start()
    LLM->>Callback: on_llm_new_token() (æµå¼)
    LLM->>Callback: on_llm_end()
    LLM-->>Chain: è¿”å›ç»“æœ
    Chain->>Callback: on_chain_end()
    Chain-->>User: æœ€ç»ˆç»“æœ

    Note over Callback: åœ¨æ¯ä¸ªé˜¶æ®µéƒ½å¯ä»¥<br/>è®°å½•æ—¥å¿—ã€ä¿®æ”¹æ•°æ®æˆ–è§¦å‘æ“ä½œ</code></pre>

                <h2>å†…ç½®å›è°ƒå¤„ç†å™¨</h2>

                <h3>1. StdOutCallbackHandler</h3>
                <p>å°†äº‹ä»¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œæœ€ç®€å•çš„è°ƒè¯•å·¥å…·ï¼š</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.callbacks import StdOutCallbackHandler

# åˆ›å»ºå¸¦æ ‡å‡†è¾“å‡ºçš„å›è°ƒ
handler = StdOutCallbackHandler()

# åœ¨ LLM ä¸Šä½¿ç”¨
llm = ChatOpenAI(model="gpt-4o")

response = llm.invoke(
    "ä»‹ç»ä¸€ä¸‹ LangChain",
    config={"callbacks": [handler]}
)
# è¾“å‡ºï¼š
# [chain/start] Entering Chain run with input: "ä»‹ç»ä¸€ä¸‹ LangChain"
# [llm/start] Entering LLM run with input: {...}
# [llm/end] Finished LLM run with output: {...}
# [chain/end] Finished Chain run</code></pre>
                </div>

                <h3>2. TokenCounterCallback</h3>
                <p>ç»Ÿè®¡ token ä½¿ç”¨æƒ…å†µï¼š</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from typing import Dict, Any

class TokenCounterCallback(BaseCallbackHandler):
    """ç»Ÿè®¡ token ä½¿ç”¨"""

    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0

    def on_llm_end(self, response: Any, **kwargs) -> None:
        """LLM è°ƒç”¨ç»“æŸæ—¶è§¦å‘"""
        # è·å– token ä½¿ç”¨æƒ…å†µ
        token_usage = response.llm_output.get("token_usage", {})

        self.prompt_tokens += token_usage.get("prompt_tokens", 0)
        self.completion_tokens += token_usage.get("completion_tokens", 0)
        self.total_tokens += token_usage.get("total_tokens", 0)

        print(f"\n--- Token ä½¿ç”¨ç»Ÿè®¡ ---")
        print(f"è¾“å…¥ tokens: {self.prompt_tokens}")
        print(f"è¾“å‡º tokens: {self.completion_tokens}")
        print(f"æ€»è®¡ tokens: {self.total_tokens}")
        print(f"é¢„ä¼°æˆæœ¬: ${self.total_tokens * 0.00001:.4f}")

# ä½¿ç”¨
counter = TokenCounterCallback()
llm = ChatOpenAI(model="gpt-4o")

response = llm.invoke(
    "å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
    config={"callbacks": [counter]}
)</code></pre>
                </div>

                <h3>3. FileCallbackHandler</h3>
                <p>å°†æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶ï¼š</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import FileCallbackHandler

# åˆ›å»ºæ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
logfile = FileCallbackHandler("langchain.log")

# åœ¨é“¾ä¸Šä½¿ç”¨
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹")
llm = ChatOpenAI(model="gpt-4o")
chain = prompt | llm

# æ‰€æœ‰æ—¥å¿—ä¼šå†™å…¥ langchain.log
chain.invoke(
    {"topic": "äººå·¥æ™ºèƒ½"},
    config={"callbacks": [logfile]}
)</code></pre>
                </div>

                <h2>è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨</h2>
                <p>åˆ›å»ºä½ è‡ªå·±çš„å›è°ƒå¤„ç†å™¨æ¥å¤„ç†ç‰¹å®šäº‹ä»¶ï¼š</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List
import json

class CustomLoggingCallback(BaseCallbackHandler):
    """è‡ªå®šä¹‰æ—¥å¿—å›è°ƒ"""

    def __init__(self, log_file: str = "execution_log.json"):
        self.log_file = log_file
        self.events = []

    def on_llm_start(
        self, prompts: List[str], **kwargs: Any
    ) -> None:
        """LLM å¼€å§‹è°ƒç”¨æ—¶"""
        event = {
            "event": "llm_start",
            "prompts": prompts,
            "timestamp": self._get_timestamp()
        }
        self.events.append(event)
        print(f"ğŸš€ LLM å¼€å§‹å¤„ç†: {prompts[0][:50]}...")

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æ–° token ç”Ÿæˆæ—¶ï¼ˆæµå¼ï¼‰"""
        print(token, end="", flush=True)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM è°ƒç”¨ç»“æŸæ—¶"""
        event = {
            "event": "llm_end",
            "output": response.generations[0][0].text,
            "token_usage": response.llm_output.get("token_usage", {}),
            "timestamp": self._get_timestamp()
        }
        self.events.append(event)
        print("\nâœ… LLM è°ƒç”¨å®Œæˆ")

    def on_chain_start(
        self, inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """é“¾å¼€å§‹æ‰§è¡Œæ—¶"""
        event = {
            "event": "chain_start",
            "inputs": inputs,
            "timestamp": self._get_timestamp()
        }
        self.events.append(event)
        print(f"â›“ï¸ é“¾å¼€å§‹æ‰§è¡Œï¼Œè¾“å…¥: {inputs}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """é“¾æ‰§è¡Œç»“æŸæ—¶"""
        event = {
            "event": "chain_end",
            "outputs": outputs,
            "timestamp": self._get_timestamp()
        }
        self.events.append(event)
        print(f"âœ… é“¾æ‰§è¡Œå®Œæˆï¼Œè¾“å‡º: {outputs}")

        # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
        self._save_log()

    def on_chain_error(
        self, error: Exception, **kwargs: Any
    ) -> None:
        """é“¾æ‰§è¡Œå‡ºé”™æ—¶"""
        event = {
            "event": "chain_error",
            "error": str(error),
            "timestamp": self._get_timestamp()
        }
        self.events.append(event)
        print(f"âŒ é“¾æ‰§è¡Œå‡ºé”™: {error}")

    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def _save_log(self) -> None:
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        with open(self.log_file, "w", encoding="utf-8") as f:
            json.dump(self.events, f, ensure_ascii=False, indent=2)

# ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
callback = CustomLoggingCallback()

llm = ChatOpenAI(model="gpt-4o")
response = llm.invoke(
    "ç”¨ä¸€å¥è¯ä»‹ç» Python",
    config={"callbacks": [callback]}
)</code></pre>
                </div>

                <h2>å›è°ƒå¤„ç†å™¨ç±»å›¾</h2>

                <pre class="mermaid">classDiagram
    class BaseCallbackHandler {
        <<abstract>>
        +on_llm_start()
        +on_llm_end()
        +on_llm_new_token()
        +on_llm_error()
        +on_chain_start()
        +on_chain_end()
        +on_chain_error()
        +on_tool_start()
        +on_tool_end()
        +on_tool_error()
        +on_chat_model_start()
        +on_chat_model_end()
        +on_chat_model_new_token()
        +on_chat_model_error()
    }

    class StdOutCallbackHandler {
        +on_llm_start()
        +on_llm_end()
        +on_chain_start()
        +on_chain_end()
    }

    class TokenCounterCallback {
        -prompt_tokens: int
        -completion_tokens: int
        -total_tokens: int
        +on_llm_end()
        +get_summary()
    }

    class CustomLoggingCallback {
        -events: list
        -log_file: str
        +on_llm_start()
        +on_llm_end()
        +on_chain_start()
        +on_chain_end()
        +on_chain_error()
        +_save_log()
    }

    BaseCallbackHandler <|-- StdOutCallbackHandler
    BaseCallbackHandler <|-- TokenCounterCallback
    BaseCallbackHandler <|-- CustomLoggingCallback</code></pre>

                <h2>åœ¨é“¾ä¸Šä½¿ç”¨å›è°ƒ</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.callbacks import BaseCallbackHandler

# å®šä¹‰å›è°ƒ
class TimingCallback(BaseCallbackHandler):
    """è®°å½•æ‰§è¡Œæ—¶é—´"""
    import time
    start_times = {}

    def on_chain_start(self, inputs, **kwargs):
        self.start_times['chain'] = self.time.time()

    def on_chain_end(self, outputs, **kwargs):
        duration = self.time.time() - self.start_times['chain']
        print(f"â±ï¸ é“¾æ‰§è¡Œè€—æ—¶: {duration:.2f}ç§’")

# åˆ›å»ºé“¾
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„{length}æ•…äº‹")
llm = ChatOpenAI(model="gpt-4o")
chain = prompt | llm

# ä½¿ç”¨å›è°ƒ
timing = TimingCallback()
response = chain.invoke(
    {"topic": "å¤ªç©ºæ¢ç´¢", "length": "ç®€çŸ­"},
    config={"callbacks": [timing]}
)</code></pre>
                </div>

                <h2>ä½¿ç”¨è£…é¥°å™¨æ·»åŠ å›è°ƒ</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import callback_handler

@callback_handler  # å°†ç±»æ³¨å†Œä¸ºå›è°ƒå¤„ç†å™¨
class MetricsCallback(BaseCallbackHandler):
    """æ”¶é›†æŒ‡æ ‡çš„å›è°ƒ"""

    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "total_tokens": 0,
            "errors": 0
        }

    def on_llm_start(self, prompts, **kwargs):
        self.metrics["total_calls"] += 1

    def on_llm_end(self, response, **kwargs):
        tokens = response.llm_output.get("token_usage", {}).get("total_tokens", 0)
        self.metrics["total_tokens"] += tokens

    def on_llm_error(self, error, **kwargs):
        self.metrics["errors"] += 1

# ä½¿ç”¨
metrics = MetricsCallback()
llm = ChatOpenAI(model="gpt-4o")

for _ in range(5):
    llm.invoke("ä½ å¥½", config={"callbacks": [metrics]})

print(f"æ€»è°ƒç”¨æ¬¡æ•°: {metrics.metrics['total_calls']}")
print(f"æ€» token æ•°: {metrics.metrics['total_tokens']}")
print(f"é”™è¯¯æ¬¡æ•°: {metrics.metrics['errors']}")</code></pre>
                </div>

                <h2>å¼‚æ­¥å›è°ƒæ”¯æŒ</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List
import asyncio

class AsyncCallbackHandler(BaseCallbackHandler):
    """å¼‚æ­¥å›è°ƒå¤„ç†å™¨"""

    # åŒæ­¥æ–¹æ³•
    def on_llm_start(self, prompts: List[str], **kwargs: Any) -> None:
        print(f"åŒæ­¥: LLM å¼€å§‹")

    # å¼‚æ­¥æ–¹æ³•
    async def on_llm_start_async(
        self, prompts: List[str], **kwargs: Any
    ) -> None:
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        print(f"å¼‚æ­¥: LLM å¼€å§‹")

    async def on_llm_end_async(self, response: LLMResult, **kwargs: Any) -> None:
        # å¯ä»¥åœ¨è¿™é‡Œæ‰§è¡Œå¼‚æ­¥æ“ä½œï¼Œå¦‚å†™å…¥æ•°æ®åº“
        await asyncio.sleep(0.1)
        print(f"å¼‚æ­¥: LLM ç»“æŸ")

# ä½¿ç”¨å¼‚æ­¥å›è°ƒ
async def main():
    from langchain_openai import AsyncChatOpenAI

    callback = AsyncCallbackHandler()
    llm = AsyncChatOpenAI(model="gpt-4o")

    response = await llm.ainvoke(
        "ä½ å¥½",
        config={"callbacks": [callback]}
    )

asyncio.run(main())</code></pre>
                </div>

                <h2>å›è°ƒç»§æ‰¿ä¸ç»„åˆ</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_core.callbacks import BaseCallbackHandler

# åŸºç¡€å›è°ƒ
class BaseMetricsCallback(BaseCallbackHandler):
    def __init__(self):
        self.metrics = {}

    def get_metrics(self):
        return self.metrics

# ç»§æ‰¿å¹¶æ‰©å±•
class DetailedMetricsCallback(BaseMetricsCallback):
    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        self.metrics.update({
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        })

# å¤šä¸ªå›è°ƒä¸€èµ·ä½¿ç”¨
from langchain_openai import ChatOpenAI

metrics = DetailedMetricsCallback()
timing = TimingCallback()

# å¯ä»¥ä¼ å…¥å¤šä¸ªå›è°ƒ
llm = ChatOpenAI(model="gpt-4o")
response = llm.invoke(
    "ä½ å¥½",
    config={"callbacks": [metrics, timing]}
)

print(metrics.get_metrics())</code></pre>
                </div>

                <h2>æœ€ä½³å®è·µ</h2>

                <div class="practice" data-label="å®è·µå»ºè®®">
                    <h3>1. åªå®ç°éœ€è¦çš„æ–¹æ³•</h3>
                    <ul>
                        <li>ä¸éœ€è¦é‡å†™æ‰€æœ‰å›è°ƒæ–¹æ³•</li>
                        <li>åªå®ç°ä½ å…³å¿ƒçš„äº‹ä»¶å¤„ç†</li>
                        <li>ä½¿ç”¨ <code>raise NotImplementedError</code> æ ‡è®°ä¸æ”¯æŒçš„å›è°ƒ</li>
                    </ul>

                    <h3>2. ä¿æŒå›è°ƒè½»é‡</h3>
                    <ul>
                        <li>é¿å…åœ¨å›è°ƒä¸­æ‰§è¡Œè€—æ—¶æ“ä½œ</li>
                        <li>è€ƒè™‘ä½¿ç”¨å¼‚æ­¥å¤„ç† IO æ“ä½œ</li>
                        <li>ä½¿ç”¨é˜Ÿåˆ—æˆ–åå°çº¿ç¨‹å¤„ç†å¤æ‚é€»è¾‘</li>
                    </ul>

                    <h3>3. é”™è¯¯å¤„ç†</h3>
                    <ul>
                        <li>å›è°ƒä¸­çš„é”™è¯¯ä¸åº”å½±å“ä¸»æµç¨‹</li>
                        <li>å®ç° <code>on_*_error</code> æ–¹æ³•å¤„ç†é”™è¯¯</li>
                        <li>è®°å½•å¼‚å¸¸ç”¨äºè°ƒè¯•</li>
                    </ul>

                    <h3>4. é¿å…æ— é™å¾ªç¯</h3>
                    <ul>
                        <li>ä¸è¦åœ¨å›è°ƒä¸­è§¦å‘ç›¸åŒçš„è°ƒç”¨</li>
                        <li>ä½¿ç”¨æ ‡å¿—ä½é˜²æ­¢é‡å¤è§¦å‘</li>
                    </ul>

                    <h3>5. æµ‹è¯•å›è°ƒ</h3>
                    <ul>
                        <li>å•å…ƒæµ‹è¯•æ¯ä¸ªå›è°ƒæ–¹æ³•</li>
                        <li>æ¨¡æ‹Ÿä¸åŒåœºæ™¯ï¼ˆæˆåŠŸã€å¤±è´¥ã€è¶…æ—¶ï¼‰</li>
                        <li>éªŒè¯å›è°ƒæ˜¯å¦æŒ‰é¢„æœŸè§¦å‘</li>
                    </ul>
                </div>

                <div class="exercise-section">
                    <h3>âœï¸ ç»ƒä¹ é¢˜</h3>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            1. å“ªä¸ªæ–¹æ³•åœ¨ LLM è°ƒç”¨å¼€å§‹æ—¶è§¦å‘ï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. on_llm_end</div>
                            <div class="option correct-option">B. on_llm_start</div>
                            <div class="option">C. on_chain_start</div>
                            <div class="option">D. on_llm_new_token</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            2. å¦‚ä½•åœ¨é“¾ä¸Šä½¿ç”¨å¤šä¸ªå›è°ƒå¤„ç†å™¨ï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. ä½¿ç”¨å›è°ƒè£…é¥°å™¨</div>
                            <div class="option correct-option">B. ä¼ å…¥åˆ—è¡¨ config={"callbacks": [cb1, cb2]}</div>
                            <div class="option">C. åªèƒ½ä½¿ç”¨ä¸€ä¸ªå›è°ƒ</div>
                            <div class="option">D. ä½¿ç”¨é“¾å¼è°ƒç”¨</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">é€‰æ‹©é¢˜</span>
                            3. å“ªä¸ªå›è°ƒæ–¹æ³•åœ¨æµå¼è¾“å‡ºæ—¶æ¯ä¸ª token ç”Ÿæˆåè§¦å‘ï¼Ÿ
                        </div>
                        <div class="options">
                            <div class="option">A. on_llm_start</div>
                            <div class="option">B. on_llm_end</div>
                            <div class="option correct-option">C. on_llm_new_token</div>
                            <div class="option">D. on_chat_model_start</div>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">ä»£ç å¡«ç©º</span>
                            4. è¡¥å…¨ä»£ç ï¼šåˆ›å»ºä¸€ä¸ªç»Ÿè®¡ API è°ƒç”¨æ¬¡æ•°çš„å›è°ƒ
                        </div>
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-language">python</span>
                                <button class="copy-btn">å¤åˆ¶</button>
                            </div>
                            <pre><code class="language-python">from langchain_core.callbacks import BaseCallbackHandler

class APICounterCallback(BaseCallbackHandler):
    def __init__(self):
        self.call_count = 0

    def on_llm_start(self, prompts, **kwargs):
        self.call_count += ____
        print(f"API è°ƒç”¨æ¬¡æ•°: {self.call_count}")

counter = APICounterCallback()
llm.invoke("ä½ å¥½", config={"callbacks": [____]})</code></pre>
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">ç¼–ç¨‹é¢˜</span>
                            5. åˆ›å»ºä¸€ä¸ªå›è°ƒå¤„ç†å™¨ï¼Œè®°å½•æ¯æ¬¡ LLM è°ƒç”¨çš„è¾“å…¥å’Œè¾“å‡ºåˆ° JSON æ–‡ä»¶
                        </div>
                    </div>

                    <div class="question">
                        <div class="question-title">
                            <span class="question-type">åœºæ™¯é¢˜</span>
                            6. ä½ æ­£åœ¨æ„å»ºä¸€ä¸ªç”Ÿäº§ç¯å¢ƒçš„ RAG åº”ç”¨ï¼Œéœ€è¦ç›‘æ§æ‰€æœ‰ç»„ä»¶çš„æ‰§è¡Œæ—¶é—´ã€token ä½¿ç”¨å’Œé”™è¯¯ç‡ã€‚ä½ ä¼šå¦‚ä½•è®¾è®¡å›è°ƒç³»ç»Ÿï¼Ÿ
                        </div>
                    </div>
                </div>

                <div class="chapter-nav">
                    <a href="12-retrieval.html">â† ä¸Šä¸€ç« ï¼šRAGæ£€ç´¢</a>
                    <a href="14-streaming.html" class="next">ä¸‹ä¸€ç« ï¼šæµå¼å¤„ç† â†’</a>
                </div>
            </div>
        </main>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script src="assets/js/main.js"></script>
</body>
</html>