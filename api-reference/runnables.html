<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Runnable æ¥å£ API - LangChain 1.0</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>

<body class="chapter-api-runnables">
    <button class="theme-toggle">ğŸŒ“</button>
    <button class="mobile-menu-btn" onclick="toggleSidebar()">â˜°</button>

    <div class="app-container">
        <nav class="sidebar">
            <div class="sidebar-header">
                <h1>LangChain 1.0</h1>
                <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 5px;">API å‚è€ƒ</p>
            </div>
            <div class="sidebar-nav"></div>
        </nav>

        <main class="main-content">
            <div class="content">
                <h1>Runnable æ¥å£ API</h1>
                <p class="subtitle">LangChain è¡¨è¾¾å¼è¯­è¨€æ ¸å¿ƒæ¥å£</p>

                <div class="api-nav">
                    <a href="#overview" class="api-nav-link">æ¦‚è¿°</a>
                    <a href="#interface" class="api-nav-link">Runnable æ¥å£</a>
                    <a href="#methods" class="api-nav-link">æ ¸å¿ƒæ–¹æ³•</a>
                    <a href="#utilities" class="api-nav-link">å·¥å…·å‡½æ•°</a>
                    <a href="#examples" class="api-nav-link">ç¤ºä¾‹</a>
                </div>

                <h2 id="overview">æ¦‚è¿°</h2>
                <p>Runnable æ˜¯ LangChain ä¸­çš„æ ¸å¿ƒæ¥å£ï¼Œæ‰€æœ‰ç»„ä»¶ï¼ˆLLMã€æç¤ºæ¨¡æ¿ã€è¾“å‡ºè§£æå™¨ç­‰ï¼‰éƒ½å®ç°äº†è¿™ä¸ªæ¥å£ï¼Œæä¾›ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼ã€‚</p>

                <pre class="mermaid">graph TD
                    A[Runnable] --> B[invoke]
                    A --> C[stream]
                    A --> D[batch]
                    A --> E[ainvoke]
                    A --> F[astream]
                    A --> G[abatch]

                    A --> H["| æ“ä½œç¬¦"]
                    A --> I[pipe]
                    A --> J[bind]
                    A --> K[with_retry]
                    A --> L[with_fallbacks]
    A --> M[map]

                    style A fill:#e1f5fe
                    style H fill:#c8e6c9</code></pre>

                <h2 id="interface">Runnable æ¥å£å®šä¹‰</h2>

                <div class="api-section">
                    <h3 class="api-title">Runnable</h3>
                    <p class="api-description">LangChain ä¸­æ‰€æœ‰å¯è¿è¡Œç»„ä»¶çš„åŸºç±»æ¥å£ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">class Runnable(Generic[Input, Output], ABC):
    """
    å¯è¿è¡Œç»„ä»¶æ¥å£

    Type Parameters:
        Input: è¾“å…¥ç±»å‹
        Output: è¾“å‡ºç±»å‹
    """

    # ========== æ ¸å¿ƒåŒæ­¥æ–¹æ³• ==========

    @abstractmethod
    def invoke(
        self,
        input: Input,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> Output:
        """
        åŒæ­¥è°ƒç”¨

        Args:
            input: è¾“å…¥æ•°æ®
            config: è¿è¡Œé…ç½®ï¼ˆcallbacks, metadata, tagsï¼‰
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            è¾“å‡ºæ•°æ®
        """

    def stream(
        self,
        input: Input,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> Iterator[Output]:
        """
        æµå¼è°ƒç”¨

        Yields:
            è¾“å‡ºæ•°æ®å—
        """

    def batch(
        self,
        inputs: List[Input],
        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
        **kwargs: Any,
    ) -> List[Output]:
        """
        æ‰¹é‡è°ƒç”¨

        Args:
            inputs: è¾“å…¥åˆ—è¡¨
            config: é…ç½®ï¼ˆå•ä¸ªæˆ–ä¸ inputs å¯¹åº”çš„åˆ—è¡¨ï¼‰
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            è¾“å‡ºåˆ—è¡¨
        """

    # ========== æ ¸å¿ƒå¼‚æ­¥æ–¹æ³• ==========

    @abstractmethod
    async def ainvoke(
        self,
        input: Input,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> Output:
        """å¼‚æ­¥è°ƒç”¨"""

    async def astream(
        self,
        input: Input,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> AsyncIterator[Output]:
        """å¼‚æ­¥æµå¼è°ƒç”¨"""

    async def abatch(
        self,
        inputs: List[Input],
        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
        **kwargs: Any,
    ) -> List[Output]:
        """å¼‚æ­¥æ‰¹é‡è°ƒç”¨"""

    # ========== ç»„åˆæ–¹æ³• ==========

    def pipe(
        self,
        other: Runnable[Any, OtherOutput],
        name: Optional[str] = None,
    ) -> Runnable[Input, OtherOutput]:
        """
        ç®¡é“è¿æ¥ï¼ˆ| æ“ä½œç¬¦å®ç°ï¼‰

        Args:
            other: ä¸‹ä¸€ä¸ª Runnable
            name: å¯é€‰åç§°

        Returns:
            ç»„åˆåçš„ Runnable
        """

    def __or__(
        self,
        other: Runnable[Any, Any],
    ) -> Runnable[Input, Any]:
        """
        | æ“ä½œç¬¦

        expression: runnable1 | runnable2
        ç­‰ä»·äº: runnable1.pipe(runnable2)
        """

    def bind(
        self,
        **kwargs: Any,
    ) -> Runnable[Input, Output]:
        """
        ç»‘å®šå‚æ•°

        Args:
            **kwargs: è¦ç»‘å®šçš„å‚æ•°

        Returns:
            ç»‘å®šå‚æ•°åçš„æ–° Runnable
        """

    def with_retry(
        self,
        *,
        retry_if_exception_type: Tuple[Type[BaseException], ...] = (Exception,),
        max_retries: int = 3,
        wait_exponential_multiplier: float = 1.0,
        wait_exponential_max: float = 10.0,
    ) -> Runnable[Input, Output]:
        """
        æ·»åŠ é‡è¯•é€»è¾‘

        Args:
            retry_if_exception_type: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait_exponential_multiplier: æŒ‡æ•°é€€é¿ä¹˜æ•°
            wait_exponential_max: æœ€å¤§ç­‰å¾…æ—¶é—´

        Returns:
            å¸¦é‡è¯•çš„ Runnable
        """

    def with_fallbacks(
        self,
        fallbacks: Sequence[Runnable[Input, Output]],
        *,
        exceptions_to_handle: Tuple[Type[BaseException], ...] = (Exception,),
    ) -> Runnable[Input, Output]:
        """
        æ·»åŠ é™çº§ç­–ç•¥

        Args:
            fallbacks: é™çº§ Runnable åˆ—è¡¨
            exceptions_to_handle: è¦å¤„ç†çš„å¼‚å¸¸ç±»å‹

        Returns:
            å¸¦é™çº§çš„ Runnable
        """

    def with_config(
        self,
        config: RunnableConfig,
    ) -> Runnable[Input, Output]:
        """
        ç»‘å®šé…ç½®

        Args:
            config: è¿è¡Œé…ç½®

        Returns:
            å¸¦é…ç½®çš„ Runnable
        """

    def with_types(
        self,
        input_type: Optional[Type[Input]] = None,
        output_type: Optional[Type[Output]] = None,
    ) -> Runnable[Input, Output]:
        """
        é…ç½®ç±»å‹

        Args:
            input_type: è¾“å…¥ç±»å‹
            output_type: è¾“å‡ºç±»å‹

        Returns:
            é…ç½®ç±»å‹åçš„ Runnable
        """

    def map(
        self,
        func: Optional[Callable[[Output], Any]] = None,
    ) -> Runnable[Input, Any]:
        """
        æ˜ å°„å‡½æ•°åˆ°è¾“å‡º

        Args:
            func: æ˜ å°„å‡½æ•°ï¼ˆNone åˆ™ä½¿ç”¨ identityï¼‰

        Returns:
            æ˜ å°„åçš„ Runnable
        """</code></pre>
                    </div>
                </div>

                <h2 id="methods">æ ¸å¿ƒæ–¹æ³•è¯¦è§£</h2>

                <div class="api-section">
                    <h3 class="api-title">RunnableConfig</h3>
                    <p class="api-description">è¿è¡Œé…ç½®ç±»å‹ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from typing import Optional, Dict, Any, List, Union
from langchain_core.callbacks import Callbacks

class RunnableConfig(TypedDict, total=False):
    """è¿è¡Œé…ç½®å­—å…¸"""

    callbacks: Callbacks
    """å›è°ƒå¤„ç†å™¨"""

    tags: List[str]
    """æ ‡ç­¾åˆ—è¡¨"""

    metadata: Dict[str, Any]
    """å…ƒæ•°æ®å­—å…¸"""

    run_name: str
    """è¿è¡Œåç§°"""

    max_concurrency: Optional[int]
    """æœ€å¤§å¹¶å‘æ•°"""

    recursion_limit: int
    """é€’å½’é™åˆ¶ï¼ˆç”¨äº RunnableBranchï¼‰"""</code></pre>
                    </div>

                    <h4>ä½¿ç”¨ç¤ºä¾‹</h4>
                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-language">python</span>
                            <button class="copy-btn">å¤åˆ¶</button>
                        </div>
                        <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.callbacks import StdOutCallbackHandler

llm = ChatOpenAI(model="gpt-4o")

# å¸¦é…ç½®è°ƒç”¨
response = llm.invoke(
    "ä½ å¥½",
    config={
        "callbacks": [StdOutCallbackHandler()],
        "tags": ["greeting", "test"],
        "metadata": {"user_id": "user_123"},
        "run_name": "greeting_generation"
    }
)</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">invoke / ainvoke</h3>
                    <p class="api-description">åŒæ­¥/å¼‚æ­¥è°ƒç”¨æ–¹æ³•ã€‚</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-language">python</span>
                            <button class="copy-btn">å¤åˆ¶</button>
                        </div>
                        <pre><code class="language-python">from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# åŒæ­¥è°ƒç”¨
response = llm.invoke("Tell me a joke")

# å¼‚æ­¥è°ƒç”¨
import asyncio

async def async_invoke():
    response = await llm.ainvoke("Tell me a joke")
    return response

asyncio.run(async_invoke())</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">stream / astream</h3>
                    <p class="api-description">æµå¼è°ƒç”¨æ–¹æ³•ã€‚</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-language">python</span>
                            <button class="copy-btn">å¤åˆ¶</button>
                        </div>
                        <pre><code class="language-python">from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# åŒæ­¥æµå¼
for chunk in llm.stream("Tell me a story"):
    print(chunk.content, end="", flush=True)

# å¼‚æ­¥æµå¼
async def async_stream():
    async for chunk in llm.astream("Tell me a story"):
        print(chunk.content, end="", flush=True)

import asyncio
asyncio.run(async_stream())</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">batch / abatch</h3>
                    <p class="api-description">æ‰¹é‡è°ƒç”¨æ–¹æ³•ã€‚</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-language">python</span>
                            <button class="copy-btn">å¤åˆ¶</button>
                        </div>
                        <pre><code class="language-python">from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# æ‰¹é‡å¤„ç†
inputs = [
    "What is Python?",
    "What is JavaScript?",
    "What is Rust?"
]
responses = llm.batch(inputs)

# å¸¦ä¸åŒé…ç½®çš„æ‰¹é‡
configs = [
    {"tags": ["question1"]},
    {"tags": ["question2"]},
    {"tags": ["question3"]},
]
responses = llm.batch(inputs, config=configs)

# å¼‚æ­¥æ‰¹é‡
async def async_batch():
    responses = await llm.abatch(inputs)
    return responses

import asyncio
asyncio.run(async_batch())</code></pre>
                    </div>
                </div>

                <h2 id="utilities">å·¥å…·å‡½æ•°</h2>

                <div class="api-section">
                    <h3 class="api-title">RunnableConfig</h3>
                    <p class="api-description">é…ç½®ç›¸å…³çš„è¾…åŠ©ç±»ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables import ensure_config, get_config

def ensure_config(
    config: Optional[RunnableConfig] = None
) -> RunnableConfig:
    """ç¡®ä¿è¿”å›æœ‰æ•ˆé…ç½®"""

def get_config() -> RunnableConfig:
    """è·å–å½“å‰è¿è¡Œä¸Šä¸‹æ–‡çš„é…ç½®"""</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">RunnableParallel</h3>
                    <p class="api-description">å¹¶è¡Œæ‰§è¡Œï¼ˆåˆ«åï¼‰ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables import RunnableParallel

# ä¸ RunnableMap ç›¸åŒ
RunnableParallel = RunnableMap</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">coalesce_to Runnable</h3>
                    <p class="api-description">å°†å¯¹è±¡è½¬æ¢ä¸º Runnableã€‚</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-language">python</span>
                            <button class="copy-btn">å¤åˆ¶</button>
                        </div>
                        <pre><code class="language-python">from langchain_core.runnables import (
    runnable_lambda,
    RunnableLambda
)

# ä¸¤ç§æ–¹å¼ç›¸åŒ
runnable_lambda(lambda x: x.upper())
RunnableLambda(lambda x: x.upper())</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">chain å‡½æ•°</h3>
                    <p class="api-description">å¿«é€Ÿåˆ›å»ºé“¾çš„å·¥å…·å‡½æ•°ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables import chain

@chain
def my_custom_chain(input: str) -> str:
    """è‡ªå®šä¹‰é“¾"""
    return input.upper()

# ä½¿ç”¨
result = my_custom_chain.invoke("hello")
# "HELLO"</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">RunnableSequence</h3>
                    <p class="api-description">åºåˆ—åŒ–æ‰§è¡Œå¤šä¸ª Runnableï¼ˆä½¿ç”¨ | æ“ä½œç¬¦ï¼‰ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables import RunnableSequence

# | æ“ä½œç¬¦åˆ›å»ºçš„å°±æ˜¯ RunnableSequence
chain = step1 | step2 | step3
# ç­‰ä»·äº:
# chain = RunnableSequence(steps=[step1, step2, step3])</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">RunnableRetry</h3>
                    <p class="api-description">å¸¦é‡è¯•çš„ Runnable åŒ…è£…å™¨ã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables import RunnableRetry

reliable_chain = RunnableRetry(
    bound=original_chain,
    max_attempts=3,
    wait_exponential_multiplier=1.0,
    wait_exponential_max=10.0
)</code></pre>
                    </div>
                </div>

                <div class="api-section">
                    <h3 class="api-title">RunnableWithMessageHistory</h3>
                    <p class="api-description">å¸¦æ¶ˆæ¯å†å²çš„ Runnableã€‚</p>

                    <div class="api-signature">
                        <pre><code class="language-python">from langchain_core.runnables.history import RunnableWithMessageHistory

chain_with_history = RunnableWithMessageHistory(
    runnable=base_chain,
    get_session_history=lambda session_id: chat_history_store[session_id],
    input_messages_key="input",
    history_messages_key="history"
)</code></pre>
                    </div>
                </div>

                <h2 id="examples">ä½¿ç”¨ç¤ºä¾‹</h2>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-language">python</span>
                        <button class="copy-btn">å¤åˆ¶</button>
                    </div>
                    <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
    RunnableBranch
)
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4o")

# ========== ç¤ºä¾‹1: åŸºç¡€ Runnable ==========
chain = ChatPromptTemplate.from_template("Tell me about {topic}") | llm | StrOutputParser()
result = chain.invoke({"topic": "LangChain"})

# ========== ç¤ºä¾‹2: RunnableLambda ==========
def process(text: str) -> str:
    return text.upper()

lambda_chain = RunnableLambda(process)
result = lambda_chain.invoke("hello")
# "HELLO"

# ========== ç¤ºä¾‹3: RunnableParallel ==========
parallel = RunnableParallel({
    "joke": ChatPromptTemplate.from_template("Tell me a joke") | llm | StrOutputParser(),
    "fact": ChatPromptTemplate.from_template("Tell me a fact") | llm | StrOutputParser(),
})
result = parallel.invoke({})

# ========== ç¤ºä¾‹4: RunnablePassthrough ==========
chain = (
    RunnablePassthrough.assign(
        processed=lambda x: x["input"].upper()
    )
)
result = chain.invoke({"input": "hello"})
# {"input": "hello", "processed": "HELLO"}

# ========== ç¤ºä¾‹5: RunnableBranch ==========
branch = RunnableBranch(
    (lambda x: x["type"] == "joke", joke_chain),
    (lambda x: x["type"] == "fact", fact_chain),
    default_chain
)

# ========== ç¤ºä¾‹6: with_retry ==========
from langchain_core.runnables import RunnableRetry

flaky_chain = llm | StrOutputParser()
reliable_chain = flaky_chain.with_retry(
    retry_if_exception_type=(ConnectionError, TimeoutError),
    max_retries=3
)

# ========== ç¤ºä¾‹7: with_fallbacks ==========
primary_chain = ChatOpenAI(model="gpt-4o") | StrOutputParser()
fallback_chain = ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()

resilient_chain = primary_chain.with_fallbacks(
    fallbacks=[fallback_chain],
    exceptions_to_handle=(RateLimitError, APIError)
)

# ========== ç¤ºä¾‹8: bind ==========
llm = ChatOpenAI(model="gpt-4o")
bound_llm = llm.bind(temperature=0.7, max_tokens=500)

# ========== ç¤ºä¾‹9: with_config ==========
chain = prompt | llm
configured_chain = chain.with_config(
    run_name="my_chain",
    tags=["production"],
    metadata={"version": "1.0"}
)

# ========== ç¤ºä¾‹10: ç»„åˆä½¿ç”¨ ==========
complex_chain = (
    RunnableParallel({
        "input": RunnablePassthrough(),
        "context": RunnableLambda(lambda x: retrieve(x["input"]))
    })
    | (lambda x: f"Context: {x['context']}\nQuestion: {x['input']}")
    | ChatPromptTemplate.from_template("{input}")
    | llm
    | StrOutputParser()
)

# ========== ç¤ºä¾‹11: å¼‚æ­¥ä½¿ç”¨ ==========
import asyncio

async def async_chain_example():
    chain = prompt | llm | StrOutputParser()

    # å¼‚æ­¥è°ƒç”¨
    result = await chain.ainvoke({"topic": "Python"})

    # å¼‚æ­¥æµå¼
    async for chunk in chain.astream({"topic": "Python"}):
        print(chunk, end="")

    # å¼‚æ­¥æ‰¹é‡
    results = await chain.abatch([
        {"topic": "Python"},
        {"topic": "JavaScript"}
    ])

asyncio.run(async_chain_example())</code></pre>
                </div>

                <h2>ç›¸å…³ API</h2>
                <ul>
                    <li><a href="chains.html">Chains API</a></li>
                    <li><a href="language-models.html">Language Models API</a></li>
                    <li><a href="../04-chains.html">æ•™ç¨‹ï¼šé“¾å¼è°ƒç”¨</a></li>
                </ul>

                <div class="chapter-nav">
                    <a href="chains.html">â† Chains API</a>
                    <a href="agents.html" class="next">Agents API â†’</a>
                </div>
            </div>
        </main>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>

</html>